                                    《训练模型》
--->定义N层神经网络
    ----->定义每层神经元的计算方式（激活函数）
        ----->初始化权重及超参数
                ----------->输入监督数据
                    -------------->求梯度
                            ---------->更新梯度
                                    ----------->计算并记录损失值
                                            ---------->评价模型训练能力精确度
----------------------------------------------------------------------------------------------------------------------------
1 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数
2 输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用softmax函数
3 分类问题中，输出层的神经元的数量设置为要分类的类别数
4 批处理可以实现高速运算
5 神经网络的训练是指从训练数据中自动获取最优权重参数的过程，最优权重参数的获取是以损失函数的最小化为基准。
当然是以所有训练数据的损失函数之和（除以N进行正规化，即求平均损失函数。跟训练集大小就没关系了）的最小化作为基准。

补充：在最小化结构风险（srm）的同时取得最优的模型参数，提高泛化能力。
训练事先指定损失值预值，求满足损失阈值前提下的的结构风险最小值。

6 深度学习的本质：通过数据（或者说是data特征）发现模式
7 数据分为训练集和测试集
    训练集：使用训练集进行学习找到最优参数；也称为监督数据。
    测试集：使用测试集评价训练得到的模型的实际能力。比如计算准确度。
    泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的
能力。提升泛化能力是机器学习的最终目标
    过拟合：只对某个数据集过度拟合的状态称为过拟合（over fitting）。
8 损失函数
    通常有均方误差和交叉熵误差
    交叉熵误差的值是由正确解标签所对应的输出结果决定的，因为tk中只有正确解标签的索引为1，其他均为0属于one-hot表示
   正确解标签对应的输出越大，值越接近0，当输出大至达到1时，交叉熵误差为0.拟合度越高。

9 mini-batch学习
    用随机选择的小批量数据（mini-batch）作为全体训练数据的近似值。然后对每个mini-batch进行学习。
    同理，训练集数据过大，如果求所有训练数据的损失函数之和则计算耗时过长。
    计算上述mini-batch小批量数据即近似值的损失函数之和。
    注意：虽然是mini-batch，但是进行多次最终近似覆盖所有训练集。

10 激活函数对损失函数的影响
比如阶跃函数的导数在大多数地方都是0，那么权重等参数的微小变化不会对最终的输出产生任何影响，这样损失函数也就不会发生变化。
而sigmod函数的导数在大多数地方都不是0，那么权重等的微小变化会对最终的输出产生连续的变化，损失函数值也会产生连续的变化，
神经网络的学习得以正常进行。

11 超参数
学习率是超参数，相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。

12 如何设置权重参数的初始值这个问题是关系到神经网络能否成功学习的重要问题
13 训练数据的损失函数值减小，是神经网络的学习正常进行的一个信号，但要注意是否过拟合
14 一个epoch表示学习中所有训练数据均被使用过一次时的更新次数

15 Sigmoid层反向传播---通过对节点进行集约化，可以不用在意Sigmoid层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要

16 通过使用层进行模块化，神经网络中可以自由地组装层，轻松构建出自己喜欢的网络
                ----------------权重的初始值-----------------
17 权重的初始值不能设置为0，是为了防止权重均一化

18 sigmoid函数的权重初始值
sigmoid激活函数输出数据：
    如果偏向0和1分布，导数值逐渐接近0，那么反向传播中梯度值也会逐渐变小最后梯度消失
    如果偏向0.5集中分布，虽然不会发生梯度消失，但是表现力受损，即每个隐藏层多个神经元输出几乎相同的值，导致下一层也会输入
相同的值，各层最后输出的激活值也分布没有适当的广度。各层间没有传递多样性的数据，那神经网络就无法进行高效的学习了。
    一般的深度学习框架中， Xavier初始值已被作为标准使用，Xavier的论文中，为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。
    推导出的结论是，如果前一层的节点数为n，则初始值使用标准差为根号n分之一的分布。
---注意：Xavier初始值是以激活函数是线性函数为前提而推导出来的


19 Relu函数的权重初始值
当激活函数使用ReLU时，权重初始值使用He初始值，即为根号n分之2

总结：神经网络的学习一定要使得权重得到有效更新，而不能因为权重梯度值过小导致更新过小甚至得不到更新，这样学习就无法进行。


20 过拟合
-发生过拟合的原因：模型拥有大量参数表现力强 、训练数据少
-抑制过拟合：
    --权值衰减法
        为损失函数加上权重的L2范数（1/2*λ*W^2）,假设有权重W = (w1, w2, . . . , wn)，则 L2范数可用根号下 w1^2 + w2^2 + …… + wn^2计算
    --dropout法
        Dropout是一种在学习的过程中随机删除神经元的方法，训练时，随机选出隐藏层的神经元，然后将其删除。
        被删除的神经元不再进行信号的传递






















