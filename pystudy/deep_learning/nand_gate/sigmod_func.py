#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@File : sigmod_func.py
@Author : jeffsheng
@Date : 2019/11/19
@Desc :
sigmod函数和阶跃函数的区别：
    1）sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化
    阶跃函数以0为界，输出发生急剧性的变化
    2）相对于阶跃函数只能返回0或1， sigmoid函数可以返回0.731 . . .、 0.880 . . .等实数
    也就是说，感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。

相似点：
    两者的结构均是“输入小时，输出接近0（为0）；随着输入增大，输出向1靠近（变成1）”。
也就是说，
1）当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；
当输入信号为不重要的信息时，两者都输出较小的值。
2） 另外，管输入信号有多小，或者有多大，输出信号的值都在0到1之间。
3） 两者均为非线性函数，sigmoid函数是一条曲线，阶跃函数是一条像阶梯一样的折线


线性函数定义：
    向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为h(x) = cx。 c为常数）。因此，线性函数是一条笔直的直线
非线性函数：指的是不像线性函数那样呈现出一条直线的函数


神经网络的激活函数必须使用非线性函数。
换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神
经网络的层数就没有意义了，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所
带来的优势，激活函数必须使用非线性函数。


"""
import numpy as np
import matplotlib.pylab as plt

"""
 参数可以是数组，原理是numpy的广播功能
 --根据NumPy 的广播功能，如果在标量和NumPy数组之间进行运算，
 则标量会和NumPy数组的各个元素进行运算
"""
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-15.0, 15.0, 0.1)
y = sigmoid(x)
#
# plt.plot(x, y)
# plt.ylim(-0.1, 1.1) # 指定y轴的范围
# plt.show()

"""
输出层softmax函数的图像:
1 softmax函数的输出是0.0到1.0之间的实数
2 输出总和为1是softmax函数的一个重要性质
"""
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
x = np.arange(0, 10.0, 1)
y = softmax(x)
plt.plot(x, y)
plt.ylim(0, 1) # 指定y轴的范围
plt.show()