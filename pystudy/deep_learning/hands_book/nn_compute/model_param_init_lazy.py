"""
前情回顾：
    在之前的model_param_get_init_share.py中
    比如只有在当我们将形状是比如(2, 20)的输入X传进网络做前向计算net(X)时，
    系统才推断出该层的权重参数形状为(256, 20)。
    因此，这时候我们才能真正开始初始化参数，也就是调用initialize函数时并没有真正初始化参数。
    在根据输入X做前向计算时，系统能够根据输入的形状自动推断出所有层的权重参数的形状。
    系统在创建这些参数之后，调用MyInit实例对它们进行初始化，然后才进行前向计算
当然，这个初始化只会在第一次前向计算时被调用。之后我们再运行前向计算net(X)时则不会重新初始化，因此不会再次产生MyInit实例的输出。
系统将真正的参数初始化延后到获得足够信息时才执行的行为叫作【延后初始化（deferred initialization）】

优点：
    它可以让模型的创建更加简单：只需要定义每个层的输出大小，而不用人工推测它们的输入个数。这对于之后将介绍的定义多达数十甚至数百层的网络来说尤其方便

弊端：
    延后初始化也可能会带来一定的困惑。在第一次前向计算之前，我们无法直接操作模型参数，
例如无法使用data函数和set_data函数来获取和修改参数。因此，我们经常会额外做一次前向计算来迫使参数被真正地初始化

------------------------------------------------------------------------------------------------------------
##避免延后初始化
# 如果系统在调用initialize函数时能够知道所有参数的形状，那么延后初始化就不会发生
第一种情况是我们要对已初始化的模型重新初始化时。因为参数形状不会发生变化，所以系统能够立即进行重新初始化
net.initialize(init=MyInit(), force_reinit=True)
第二种情况是我们在创建层的时候指定了它的输入个数，使系统不需要额外的信息来推测参数形状
通过in_units来指定每个全连接层的输入个数，使初始化能够在initialize函数被调用时立即发生
        net = nn.Sequential()
        net.add(nn.Dense(256, in_units=20, activation='relu'))
        net.add(nn.Dense(10, in_units=256))

        net.initialize(init=MyInit())
"""
