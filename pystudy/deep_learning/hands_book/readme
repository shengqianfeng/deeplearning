《动手学深度学习》基于tensorflow实现：https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/
B站视频：https://space.bilibili.com/209599371

安装torch：pip install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html

1 基于softmax和线性回归的是单层神经网络
2 全连接层如果不用激活函数，无论层再多也相当于是一个单层神经网络。因为全连接层只是对数据做仿射变换，而多个仿射变换的叠加仍然是一个仿射变换。
而激活函数一般是非线性变换。
3 多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换

4 应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型；
另一个办法增加训练数据集，一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。
此外，泛化误差不会随训练数据集里样本数量增加而增大。
因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。

5 权重衰减(L2范数正则化)
    L2范数正则化令权重 w1 和 w2先自乘小于1的数，再减去不含惩罚项的梯度。因此， L2范数正则化又叫权重衰减
权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效.

6 丢弃法应对过拟合，只在训练模型时使用

7 当神经网络的层数较多时，模型的数值稳定性容易变差，输出容易出现衰减或爆炸。

8 隐藏层权重参数随机初始化的原因：如果隐藏层每个神经元的参数都初始化为相同的值，则隐藏层每个神经元正向传播后续输出都相等。反向传播梯度更新也相等。
这样的话，不论隐藏层的神经元有多少个，这些神经元的作用都是等价的，相当于只有一个神经元在起作用。所以通常将神经网络模型参数，尤其是权重参数随机初始化。
ps：（第8点寡人尚未作论证，且用之）
-8.1 Tensorflow中initializers的模块参数都采取了较为合理的初始化策略（比如正态分布等随机方式），因此一般我们不用考虑。
-8.2 另一种比较常用的随机初始化方法叫作Xavier随机初始化，Xavier随机初始化将使该隐藏层中权重参数的每个元素都随机采样于均匀分布，它的设计主要考虑到，模型参数初始化后，
每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。

9 在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出

10 我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

11 使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大

12 LeNet、AlexNet、VGG、NiN比较
    LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果
    AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。
    NiN提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络

13 卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）
























原书地址：http://zh.d2l.ai/chapter_how-to-use/how-to-use.html
要点：
1 关注神经元计算的输入输出
2 关注是否使用广播，比如偏置的加法运算
3 关注数学公式是核心原理
4 边走读代码边加注释，最好别放过任何一个小知识点，比如api的用法
